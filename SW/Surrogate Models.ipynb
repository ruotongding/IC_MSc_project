{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bu8o9SBEkO30"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pylab import *\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmlUlMiNkwdW",
        "outputId": "6a4d3a4a-3260-4f02-a573-1b456b4e1835"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "content_path = '/content/gdrive/MyDrive/ic_project/new'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S0SnKvqoloqO"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O3OiRbTGXJYa"
      },
      "outputs": [],
      "source": [
        "\n",
        "#construct background states, observations with error\n",
        "\n",
        "def x_to_y(X): # averaging in 2*2 windows (4 pixels)\n",
        "    dim = X.shape[0]\n",
        "    dim = 20\n",
        "    Y = np.zeros((int(dim/2),int(dim/2)))\n",
        "    for i in range(int(dim/2)):\n",
        "        for j in range(int(dim/2)):\n",
        "            Y[i,j] = X[2*i,2*j] + X[2*i+1,2*j] + X[2*i,2*j+1] + X[2*i+1,2*j+1]\n",
        "\n",
        "            Y_noise = np.random.multivariate_normal(np.zeros(100),0.0000 * np.eye(100))\n",
        "            Y_noise.shape = (10,10)\n",
        "            Y = Y + Y_noise\n",
        "    return Y\n",
        "\n",
        "\n",
        "class shallow(object):\n",
        "\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    plt = []\n",
        "    fig = []\n",
        "\n",
        "\n",
        "    def __init__(self, x=[],y=[],h_ini = 1.,u=[],v = [],dx=0.01,dt=0.0001, N=64,L=1., px=16, py=16, R=64, Hp=0.1, g=1., b=0.): # How define no default argument before?\n",
        "\n",
        "\n",
        "        # add a perturbation in pressure surface\n",
        "\n",
        "\n",
        "        self.px, self.py = px, py\n",
        "        self.R = R\n",
        "        self.Hp = Hp\n",
        "\n",
        "\n",
        "\n",
        "        # Physical parameters\n",
        "\n",
        "        self.g = g\n",
        "        self.b = b\n",
        "        self.L=L\n",
        "        self.N=N\n",
        "\n",
        "        # limits for h,u,v\n",
        "\n",
        "\n",
        "        #self.dx =  self.L / self.N # a changer\n",
        "        #self.dt = self.dx / 100.\n",
        "        self.dx=dx\n",
        "        self.dt=dt\n",
        "\n",
        "        self.x,self.y = mgrid[:self.N,:self.N]\n",
        "\n",
        "        self.u=zeros((self.N,self.N))\n",
        "        self.v=zeros((self.N,self.N))\n",
        "\n",
        "        self.h_ini=h_ini\n",
        "\n",
        "        self.h=self.h_ini * ones((self.N,self.N))\n",
        "\n",
        "        rr = (self.x-px)**2 + (self.y-py)**2\n",
        "        self.h[rr<R] = self.h_ini + Hp #set initial conditions\n",
        "\n",
        "        self.lims = [(self.h_ini-self.Hp,self.h_ini+self.Hp),(-0.02,0.02),(-0.02,0.02)]\n",
        "\n",
        "\n",
        "\n",
        "    def dxy(self, A, axis=0):\n",
        "        \"\"\"\n",
        "        Compute derivative of array A using balanced finite differences\n",
        "        Axis specifies direction of spatial derivative (d/dx or d/dy)\n",
        "        dA[i]/dx =  (A[i+1] - A[i-1] )  / 2dx\n",
        "        \"\"\"\n",
        "        return (roll(A, -1, axis) - roll(A, 1, axis)) / (self.dx*2.) # roll: shift the array axis=0 shift the horizontal axis\n",
        "\n",
        "    def d_dx(self, A):\n",
        "        return self.dxy(A,1)\n",
        "\n",
        "    def d_dy(self, A):\n",
        "        return self.dxy(A,0)\n",
        "\n",
        "\n",
        "    def d_dt(self, h, u, v):\n",
        "        \"\"\"\n",
        "        http://en.wikipedia.org/wiki/Shallow_water_equations#Non-conservative_form\n",
        "        \"\"\"\n",
        "        for x in [h, u, v]: # type check\n",
        "           assert isinstance(x, ndarray) and not isinstance(x, matrix)\n",
        "\n",
        "        g,b,dx = self.g, self.b, self.dx\n",
        "\n",
        "        du_dt = -g*self.d_dx(h) - b*u\n",
        "        dv_dt = -g*self.d_dy(h) - b*v\n",
        "\n",
        "        H = 0 #h.mean() - our definition of h includes this term\n",
        "        dh_dt = -self.d_dx(u * (H+h)) - self.d_dy(v * (H+h))\n",
        "\n",
        "        return dh_dt, du_dt, dv_dt\n",
        "\n",
        "\n",
        "    def evolve(self):\n",
        "        \"\"\"\n",
        "        Evolve state (h, u, v) forward in time using simple Euler method\n",
        "        x_{N+1} = x_{N} +   dx/dt * d_t\n",
        "        \"\"\"\n",
        "\n",
        "        dh_dt, du_dt, dv_dt = self.d_dt(self.h, self.u, self.v)\n",
        "        dt = self.dt\n",
        "\n",
        "        self.h += dh_dt * dt\n",
        "        self.u += du_dt * dt\n",
        "        self.v += dv_dt * dt\n",
        "        self.time += dt\n",
        "\n",
        "        return self.h, self.u, self.v\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtUh8sK9GbVV",
        "outputId": "37790b31-6481-4dcd-c6e2-d6d4222edca0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate the dataset\n",
        "def generate_dataset(num_simulations=500, snapshots_per_simulation=200, N=64):\n",
        "    dataset = np.zeros((num_simulations * (snapshots_per_simulation//50), 3, N, N))\n",
        "    first_snapshots = np.zeros((num_simulations, 3, N, N))\n",
        "    paras = np.zeros((num_simulations * snapshots_per_simulation, 5))\n",
        "    snapshot_idx = 0\n",
        "\n",
        "    for sim in range(num_simulations):\n",
        "\n",
        "        px=random.randint(54, 74)*1.\n",
        "        py=random.randint(54, 74)*1.\n",
        "        R=random.randint(80, 160)*1.\n",
        "        Hp=random.randint(5, 20)*0.01\n",
        "        b=random.randint(1, 100)*0.1\n",
        "\n",
        "        SW = shallow(N=N, px=px, py=py, R=R, Hp=Hp, b=b)\n",
        "\n",
        "        for step in range(snapshots_per_simulation):\n",
        "            SW.evolve()\n",
        "            if (step)%50==0:\n",
        "              dataset[snapshot_idx, 0, :, :] = SW.u\n",
        "              dataset[snapshot_idx, 1, :, :] = SW.v\n",
        "              dataset[snapshot_idx, 2, :, :] = SW.h\n",
        "              paras[snapshot_idx, 0] = px\n",
        "              paras[snapshot_idx, 1] = py\n",
        "              paras[snapshot_idx, 2] = R\n",
        "              paras[snapshot_idx, 3] = Hp\n",
        "              paras[snapshot_idx, 4] = b\n",
        "              snapshot_idx += 1\n",
        "              # Save the first snapshot of the simulation\n",
        "              if step == snapshots_per_simulation-1:\n",
        "                  first_snapshots[sim, 0, :, :] = SW.u\n",
        "                  first_snapshots[sim, 1, :, :] = SW.v\n",
        "                  first_snapshots[sim, 2, :, :] = SW.h\n",
        "    return dataset,first_snapshots,paras\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def min_max_normalize(dataset):\n",
        "    u_min = dataset[:, 0, :, :].min()\n",
        "    u_max = dataset[:, 0, :, :].max()\n",
        "    v_min = dataset[:, 1, :, :].min()\n",
        "    v_max = dataset[:, 1, :, :].max()\n",
        "    h_min = dataset[:, 2, :, :].min()\n",
        "    h_max = dataset[:, 2, :, :].max()\n",
        "\n",
        "    dataset[:, 0, :, :] = (dataset[:, 0, :, :] - u_min) / (u_max - u_min)\n",
        "    dataset[:, 1, :, :] = (dataset[:, 1, :, :] - v_min) / (v_max - v_min)\n",
        "    dataset[:, 2, :, :] = (dataset[:, 2, :, :] - h_min) / (h_max - h_min)\n",
        "\n",
        "    return dataset, [u_min, u_max, v_min, v_max, h_min, h_max]\n",
        "\n",
        "# Generate the dataset and save it\n",
        "dataset,first_snapshots,paras = generate_dataset(num_simulations=100, snapshots_per_simulation=5000, N=128)\n",
        "dataset, normalization_params= min_max_normalize(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_IW9YgDeLiz"
      },
      "source": [
        "construct 5 to 5 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJt9nfRGV8BR",
        "outputId": "ea6383ee-3df9-4702-e973-61d14d853fa0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Number of simulations and snapshots per simulation\n",
        "num_simulations = 100\n",
        "snapshots_per_simulation = 100\n",
        "slice_steps = 5\n",
        "interval = 3\n",
        "N = 128\n",
        "\n",
        "# Initialize arrays for slices and their corresponding targets\n",
        "num_slices_per_simulation = snapshots_per_simulation - (2 * slice_steps-1) * interval\n",
        "total_slices = num_simulations * num_slices_per_simulation\n",
        "\n",
        "slices = np.zeros((total_slices, slice_steps, 3, N, N))\n",
        "targets = np.zeros((total_slices, slice_steps, 3, N, N))\n",
        "slice_paras = np.zeros((total_slices, slice_steps,5))\n",
        "\n",
        "# Generate slices and targets\n",
        "slice_idx = 0\n",
        "for sim in range(num_simulations):\n",
        "    for start in range(num_slices_per_simulation):\n",
        "        sim_start_idx = sim * snapshots_per_simulation\n",
        "        input_indices = [sim_start_idx + start + i * interval for i in range(slice_steps)]\n",
        "        target_indices = [sim_start_idx + start + slice_steps * interval + i * interval for i in range(slice_steps)]\n",
        "\n",
        "        # Ensure indices are within the simulation range\n",
        "        if target_indices[-1] < (sim + 1) * snapshots_per_simulation:\n",
        "            slices[slice_idx] = dataset[input_indices]\n",
        "            targets[slice_idx] = dataset[target_indices]\n",
        "            slice_paras[slice_idx] = paras[input_indices]\n",
        "            slice_idx += 1\n",
        "\n",
        "# Remove unused portion of arrays if any\n",
        "slices = slices[:slice_idx]\n",
        "targets = targets[:slice_idx]\n",
        "train_ratio = 0.8\n",
        "num_samples = targets.shape[0]\n",
        "num_train = int(num_samples * train_ratio)\n",
        "targets_test_oriimg = torch.tensor(targets[num_train:], dtype=torch.float32).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PpiFoBzDVpgn"
      },
      "outputs": [],
      "source": [
        "class KANLinear(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        enable_standalone_scale_spline=True,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = (\n",
        "            (\n",
        "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                + grid_range[0]\n",
        "            )\n",
        "            .expand(in_features, -1)\n",
        "            .contiguous()\n",
        "        )\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = torch.nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = torch.nn.Parameter(\n",
        "                torch.Tensor(out_features, in_features)\n",
        "            )\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = (\n",
        "                (\n",
        "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
        "                    - 1 / 2\n",
        "                )\n",
        "                * self.scale_noise\n",
        "                / self.grid_size\n",
        "            )\n",
        "            self.spline_weight.data.copy_(\n",
        "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
        "                * self.curve2coeff(\n",
        "                    self.grid.T[self.spline_order : -self.spline_order],\n",
        "                    noise,\n",
        "                )\n",
        "            )\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
        "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the B-spline bases for the given input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "\n",
        "        grid: torch.Tensor = (\n",
        "            self.grid\n",
        "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the coefficients of the curve that interpolates the given points.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "\n",
        "        A = self.b_splines(x).transpose(\n",
        "            0, 1\n",
        "        )  # (in_features, batch_size, grid_size + spline_order)\n",
        "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
        "        solution = torch.linalg.lstsq(\n",
        "            A, B\n",
        "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
        "        result = solution.permute(\n",
        "            2, 0, 1\n",
        "        )  # (out_features, in_features, grid_size + spline_order)\n",
        "\n",
        "        assert result.size() == (\n",
        "            self.out_features,\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        return self.spline_weight * (\n",
        "            self.spline_scaler.unsqueeze(-1)\n",
        "            if self.enable_standalone_scale_spline\n",
        "            else 1.0\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.size(-1) == self.in_features\n",
        "        original_shape = x.shape\n",
        "        x = x.reshape(-1, self.in_features)\n",
        "\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.scaled_spline_weight.view(self.out_features, -1),\n",
        "        )\n",
        "        output = base_output + spline_output\n",
        "\n",
        "        output = output.reshape(*original_shape[:-1], self.out_features)\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "\n",
        "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
        "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
        "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
        "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
        "        unreduced_spline_output = unreduced_spline_output.permute(\n",
        "            1, 0, 2\n",
        "        )  # (batch, in, out)\n",
        "\n",
        "        # sort each channel individually to collect data distribution\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[\n",
        "            torch.linspace(\n",
        "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (\n",
        "            torch.arange(\n",
        "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
        "            ).unsqueeze(1)\n",
        "            * uniform_step\n",
        "            + x_sorted[0]\n",
        "            - margin\n",
        "        )\n",
        "\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.concatenate(\n",
        "            [\n",
        "                grid[:1]\n",
        "                - uniform_step\n",
        "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
        "                grid,\n",
        "                grid[-1:]\n",
        "                + uniform_step\n",
        "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        \"\"\"\n",
        "        Compute the regularization loss.\n",
        "\n",
        "        This is a dumb simulation of the original L1 regularization as stated in the\n",
        "        paper, since the original one requires computing absolutes and entropy from the\n",
        "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
        "        behind the F.linear function if we want an memory efficient implementation.\n",
        "\n",
        "        The L1 regularization is now computed as mean absolute value of the spline\n",
        "        weights. The authors implementation also includes this term in addition to the\n",
        "        sample-based regularization.\n",
        "        \"\"\"\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return (\n",
        "            regularize_activation * regularization_loss_activation\n",
        "            + regularize_entropy * regularization_loss_entropy\n",
        "        )\n",
        "\n",
        "\n",
        "class KAN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers_hidden,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KAN, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
        "            self.layers.append(\n",
        "                KANLinear(\n",
        "                    in_features,\n",
        "                    out_features,\n",
        "                    grid_size=grid_size,\n",
        "                    spline_order=spline_order,\n",
        "                    scale_noise=scale_noise,\n",
        "                    scale_base=scale_base,\n",
        "                    scale_spline=scale_spline,\n",
        "                    base_activation=base_activation,\n",
        "                    grid_eps=grid_eps,\n",
        "                    grid_range=grid_range,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, update_grid=False):\n",
        "        for layer in self.layers:\n",
        "            if update_grid:\n",
        "                layer.update_grid(x)\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        return sum(\n",
        "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
        "            for layer in self.layers\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define architecture of CAE-KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i8GRnrpnDuK-"
      },
      "outputs": [],
      "source": [
        "class PowerfulAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(PowerfulAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # (128, 128, 3) -> (64, 64, 32)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # (64, 64, 32) -> (32, 32, 64)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # (32, 32, 64) -> (16, 16, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # (16, 16, 128) -> (8, 8, 256)\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            # nn.Linear(8*8*256, latent_dim)  # (8*8*256) -> (latent_dim)\n",
        "            KANLinear(8*8*256, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8*8*256),  # (latent_dim) -> (8*8*256)\n",
        "            nn.Unflatten(1, (256, 8, 8)),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (8, 8, 256) -> (16, 16, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (16, 16, 128) -> (32, 32, 64)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # (32, 32, 64) -> (64, 64, 32)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1), # (64, 64, 32) -> (128, 128, 3)\n",
        "            nn.Sigmoid()  # Use sigmoid if your image pixels are in the range [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        return x_reconstructed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHMV6hORMhkt",
        "outputId": "4e659e60-8924-4eb5-fa92-3d82b24fa7a4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "model_path = content_path + '/AE_KAN.pth'\n",
        "latent_dim = 16\n",
        "\n",
        "# Initialize the model and load the trained parameters\n",
        "Autoencoder = PowerfulAutoencoder(latent_dim=latent_dim).to(device)\n",
        "Autoencoder.load_state_dict(torch.load(model_path))\n",
        "Autoencoder.eval()  # Set the model to evaluation mode\n",
        "\n",
        "slices_tensor = torch.tensor(slices, dtype=torch.float32).to(device)\n",
        "targets_tensor = torch.tensor(targets, dtype=torch.float32).to(device)\n",
        "slice_paras = torch.tensor(slice_paras, dtype=torch.float32).to(device)\n",
        "print(slices_tensor)\n",
        "\n",
        "# Get the number of slices\n",
        "num_slices = slices_tensor.shape[0]\n",
        "\n",
        "# Initialize arrays to store latent vectors\n",
        "slices_latent = torch.zeros((num_slices, slice_steps, latent_dim)).to(device)\n",
        "targets_latent = torch.zeros((num_slices, slice_steps, latent_dim)).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(num_slices):\n",
        "        for j in range(slice_steps):\n",
        "\n",
        "            slices_latent[i, j] = Autoencoder.encoder(slices_tensor[i, j].unsqueeze(0))\n",
        "            targets_latent[i, j] = Autoencoder.encoder(targets_tensor[i, j].unsqueeze(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"slices_latent\",slices_latent.shape)\n",
        "print(\"targetss_latent\",targets_latent.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Surrogate Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K2qyYPmRmxx9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims,base_activation = nn.ReLU()):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(input_dims) - 1):\n",
        "            layers.append(nn.Linear(input_dims[i], input_dims[i+1]))\n",
        "            if i < len(input_dims) - 2:  # No activation function on the last layer\n",
        "                layers.append(base_activation)\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "#LSTM for 16\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout = 0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,dropout = dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
        "\n",
        "        # Initializing cell state for first input with zeros\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# GRU for 16\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "\n",
        "        out, hn = self.gru(x, h0.detach())\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7Gk0KoFWm3V7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, path='checkpoint.pth'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        torch.save(model.state_dict(), self.path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ml4KeqasJM0",
        "outputId": "bfedadd2-04af-412f-b8d9-df13999d56f9"
      },
      "outputs": [],
      "source": [
        "slices_tensor = torch.tensor(slices_latent, dtype=torch.float32).to(device)\n",
        "targets_tensor = torch.tensor(targets_latent, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "seed = 5\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "train_ratio = 0.8\n",
        "num_samples = slices_tensor.shape[0]\n",
        "num_train = int(num_samples * train_ratio)\n",
        "num_test = num_samples - num_train\n",
        "train_slices = slices_tensor[:num_train]\n",
        "\n",
        "\n",
        "train_paras = slice_paras[:num_train]\n",
        "train_targets = targets_tensor[:num_train]\n",
        "test_slices = slices_tensor[num_train:]\n",
        "\n",
        "\n",
        "test_targets = targets_tensor[num_train:]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_slices, train_targets,train_paras)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_slices, test_targets)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model,RNN_flag, train_loader,test_loader,early_stopping):\n",
        "\n",
        "  # mlp = MLP(input_dim, output_dim).to(device)\n",
        "  criterion = nn.MSELoss()\n",
        "  # optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "  # Define learning rate scheduler\n",
        "  # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "  beta = 0.01\n",
        "  # Training loop\n",
        "  num_epochs = 30\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  stop_epochs = num_epochs\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      train_loss = 0\n",
        "      for data in train_loader:\n",
        "          inputs, targets,klg = data\n",
        "          optimizer.zero_grad()\n",
        "          #mlp and KAN\n",
        "          if RNN_flag == False:\n",
        "            flatten_inputs = inputs.reshape(inputs.shape[0],inputs.shape[1]*inputs.shape[2])\n",
        "            flatten_outputs = model(flatten_inputs)\n",
        "            outputs = flatten_outputs.reshape(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
        "          elif RNN_flag == True:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "\n",
        "      train_loss /= len(train_loader)\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      model.eval()\n",
        "      test_loss = 0\n",
        "      with torch.no_grad():\n",
        "          for data in test_loader:\n",
        "              inputs, targets = data\n",
        "\n",
        "              if RNN_flag == False:\n",
        "                flatten_inputs = inputs.reshape(inputs.shape[0],inputs.shape[1]*inputs.shape[2])\n",
        "                flatten_outputs = model(flatten_inputs)\n",
        "                outputs = flatten_outputs.reshape(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
        "\n",
        "\n",
        "              elif RNN_flag == True:\n",
        "                outputs = model(inputs)\n",
        "\n",
        "\n",
        "              loss = criterion(outputs, targets)\n",
        "              test_loss += loss.item()\n",
        "\n",
        "      test_loss /= len(test_loader)\n",
        "      test_losses.append(test_loss)\n",
        "      scheduler.step(test_loss)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}')\n",
        "      #Check early stopping condition\n",
        "      early_stopping(test_loss,model)\n",
        "      if early_stopping.early_stop:\n",
        "\n",
        "          print(\"Early stopping\")\n",
        "          print(\"best_loss\",early_stopping.best_loss)\n",
        "          break\n",
        "\n",
        "\n",
        "  print('Training complete')\n",
        "\n",
        "  # Save the trained model\n",
        "  # model_save_path = content_path + '/kan.pth'\n",
        "  # torch.save(kan.state_dict(), model_save_path)\n",
        "  # print(f'Model saved to {model_save_path}')\n",
        "\n",
        "  # # Plot the training and testing losses\n",
        "  # plt.figure(figsize=(10, 5))\n",
        "  # plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
        "  # plt.plot(range(1, len(test_losses)+1), test_losses, label='Test Loss')\n",
        "  # plt.xlabel('Epoch')\n",
        "  # plt.ylabel('Loss')\n",
        "  # plt.title('Training and Testing Loss per Epoch')\n",
        "  # plt.legend()\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wFetzwfG7Vpl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the function to calculate RRMSE\n",
        "def rrmse(img1, img2):\n",
        "    return np.sqrt(np.mean((img1 - img2) ** 2)) / np.sqrt(np.mean(img1 ** 2))\n",
        "\n",
        "# def compute_ssim(img1, img2):\n",
        "#     return ssim(img1, img2, data_range=img2.max() - img2.min(), multichannel=True)\n",
        "def compute_ssim(img1, img2):\n",
        "    return ssim(img1, img2, data_range=img2.max() - img2.min(), channel_axis=2)\n",
        "\n",
        "def compute_psnr(img1, img2):\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    max_pixel = 1.0\n",
        "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "def plot_images(output_img, error_img):\n",
        "    # Plot the first channel of the output image in a separate figure\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    cax0 = plt.imshow(output_img, cmap='viridis')\n",
        "    plt.title('')\n",
        "    plt.axis('off')\n",
        "    plt.colorbar(cax0)  # Add colorbar for the output image\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the error heatmap in a separate figure\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    cax1 = plt.imshow(error_img, cmap='viridis')\n",
        "    plt.title('')\n",
        "    plt.axis('off')\n",
        "    plt.colorbar(cax1)  # Add colorbar for the heatmap\n",
        "    plt.show()\n",
        "\n",
        "def gaim_metrics(test_loader, Autoencoder, model, RNN_flag):\n",
        "\n",
        "  # Ensure the model is in evaluation mode\n",
        "  model.eval()\n",
        "  print(model)\n",
        "  Autoencoder.eval()\n",
        "\n",
        "  # Decode the LSTM predictions\n",
        "  decoded_outputs = torch.zeros((len(test_loader.dataset), slice_steps, 3, N, N)).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      global_idx = 0\n",
        "      for idx, data in enumerate(test_loader):\n",
        "          inputs, targets = data\n",
        "          batch_size = inputs.size(0)\n",
        "          if RNN_flag == False:\n",
        "            flatten_inputs = inputs.reshape(inputs.shape[0],inputs.shape[1]*inputs.shape[2])\n",
        "            flatten_outputs = model(flatten_inputs)\n",
        "            outputs_encoded = flatten_outputs.reshape(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
        "\n",
        "\n",
        "          elif RNN_flag == True:\n",
        "            inputs = inputs\n",
        "\n",
        "            outputs_encoded = model(inputs)\n",
        "          # Decode the LSTM predictions\n",
        "          for i in range(batch_size):\n",
        "\n",
        "              for j in range(slice_steps):\n",
        "                  decoded_outputs[global_idx + i, j] = Autoencoder.decoder(outputs_encoded[i, j].unsqueeze(0)).squeeze(0)\n",
        "\n",
        "          global_idx += batch_size\n",
        "\n",
        "\n",
        "  # Loop through the decoded outputs and calculate RRMSE and SSIM\n",
        "  rrmse_values = []\n",
        "  ssim_values = []\n",
        "  mse_values = []\n",
        "  psnr_values = []\n",
        "  with torch.no_grad():\n",
        "      for i in range(len(decoded_outputs)):\n",
        "          for j in range(slice_steps):\n",
        "              target_img = targets_test_oriimg[i, j].cpu().numpy()\n",
        "              output_img = decoded_outputs[i, j].cpu().numpy()\n",
        "              target_img = target_img.transpose((1, 2, 0))\n",
        "              output_img = output_img.transpose((1, 2, 0))\n",
        "\n",
        "\n",
        "              target_img_flatten = target_img.flatten()\n",
        "              output_img_flatten = output_img.flatten()\n",
        "              # Compute the Mean Squared Error\n",
        "              mse = np.mean((target_img_flatten - output_img_flatten) ** 2)\n",
        "              mse_values.append(mse)\n",
        "\n",
        "              rrmse_value = rrmse(target_img,output_img).item()\n",
        "              rrmse_values.append(rrmse_value)\n",
        "              # Calculate SSIM for each channel and average them\n",
        "              ssim_value = compute_ssim(target_img, output_img)\n",
        "              ssim_values.append(ssim_value)\n",
        "              psnr_value = compute_psnr(target_img, output_img)\n",
        "              psnr_values.append(psnr_value)\n",
        "              if i == 100 and j == 3:\n",
        "                target_img = targets_test_oriimg[i, j].cpu().numpy()\n",
        "                output_img = decoded_outputs[i, j].cpu().numpy()\n",
        "\n",
        "                # Transpose images back to their original dimensions\n",
        "                target_img = target_img.transpose((1, 2, 0))\n",
        "                output_img = output_img.transpose((1, 2, 0))\n",
        "\n",
        "                # Select only the first channel (channel 0)\n",
        "                target_img_channel_0 = target_img[:, :, 0]\n",
        "                output_img_channel_0 = output_img[:, :, 0]\n",
        "\n",
        "                # Calculate the absolute error for channel 0\n",
        "                error_img = np.abs(target_img_channel_0 - output_img_channel_0)\n",
        "\n",
        "                # Plot the output image (channel 0) and error heatmap using 'viridis'\n",
        "                plot_images(output_img_channel_0, error_img)\n",
        "  return rrmse_values,ssim_values,mse_values,psnr_values,decoded_outputs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ3jK2QbHKZM"
      },
      "source": [
        "Roll out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wm6eWPi8G9tX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rollout_metrics(test_loader, Autoencoder, model, RNN_flag):\n",
        "  # Initialize lists to store RRMSE and SSIM scores\n",
        "\n",
        "  # Ensure the model is in evaluation mode\n",
        "  # model.load_state_dict(torch.load(model_save_path))\n",
        "  model.eval()\n",
        "  print(model)\n",
        "  Autoencoder.eval()\n",
        "\n",
        "\n",
        "  # Decode the LSTM predictions\n",
        "  decoded_outputs = torch.zeros((len(test_loader.dataset), slice_steps, 3, N, N)).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      global_idx = 0\n",
        "      for idx, data in enumerate(test_loader):\n",
        "          inputs, targets = data\n",
        "\n",
        "          batch_size = inputs.size(0)\n",
        "          if RNN_flag == False:\n",
        "            first_input = inputs[0]\n",
        "            model_input = first_input.unsqueeze(0)\n",
        "            outputs_encoded = torch.zeros((batch_size, slice_steps, 16)).to(device)\n",
        "            model_input = model_input.reshape(model_input.shape[0],model_input.shape[1]*model_input.shape[2])\n",
        "            #apply roll out\n",
        "            for b in range(batch_size):\n",
        "\n",
        "              model_out = model(model_input.unsqueeze(0))\n",
        "              output_reshape = model_out.reshape(1,5,16)\n",
        "              outputs_encoded[b] = output_reshape.squeeze(0)\n",
        "              model_input = model_out.squeeze(0)\n",
        "\n",
        "          elif RNN_flag == True:\n",
        "            first_input = inputs[0]\n",
        "            model_input = first_input\n",
        "            outputs_encoded = torch.zeros((batch_size, slice_steps, 16)).to(device)\n",
        "            for b in range(batch_size):\n",
        "\n",
        "              model_out = model(model_input.unsqueeze(0))\n",
        "              outputs_encoded[b] = model_out.squeeze(0)\n",
        "              model_input = model_out.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Decode the model predictions\n",
        "          for i in range(batch_size):\n",
        "              for j in range(slice_steps):\n",
        "                  decoded_outputs[global_idx + i, j] = Autoencoder.decoder(outputs_encoded[i, j].unsqueeze(0)).squeeze(0)\n",
        "\n",
        "          global_idx += batch_size\n",
        "\n",
        "  # Loop through the decoded outputs and calculate metrics\n",
        "  rrmse_values = []\n",
        "  ssim_values = []\n",
        "  mse_values = []\n",
        "  psnr_values = []\n",
        "  with torch.no_grad():\n",
        "      for i in range(len(decoded_outputs)):\n",
        "          for j in range(slice_steps):\n",
        "              target_img = targets_test_oriimg[i, j].cpu().numpy()\n",
        "              output_img = decoded_outputs[i, j].cpu().numpy()\n",
        "              target_img = target_img.transpose((1, 2, 0))\n",
        "              output_img = output_img.transpose((1, 2, 0))\n",
        "\n",
        "\n",
        "              target_img_flatten = target_img.flatten()\n",
        "              output_img_flatten = output_img.flatten()\n",
        "              mse = np.mean((target_img_flatten - output_img_flatten) ** 2)\n",
        "              mse_values.append(mse)\n",
        "              rrmse_value = rrmse(target_img,output_img).item()\n",
        "              rrmse_values.append(rrmse_value)\n",
        "              ssim_value = compute_ssim(target_img, output_img)\n",
        "              ssim_values.append(ssim_value)\n",
        "              psnr_value = compute_psnr(target_img, output_img)\n",
        "              psnr_values.append(psnr_value)\n",
        "  return rrmse_values,ssim_values,mse_values,psnr_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw5ZoESwbDOW"
      },
      "source": [
        "KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hYO0pqcZbEkC",
        "outputId": "c5965c44-0d45-41b5-ed67-6b2a03173fe3"
      },
      "outputs": [],
      "source": [
        "input_dim = 5 * 16\n",
        "output_dim = 5 * 16\n",
        "input_dims = [input_dim,256,output_dim]  # Example hidden layers\n",
        "\n",
        "kan = KAN(input_dims).to(device)\n",
        "RNN_flag = False\n",
        "print(kan)\n",
        "model_save_path = content_path + 'kan.pth'\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=1e-5, path=model_save_path)\n",
        "train(kan,RNN_flag, train_loader,test_loader,early_stopping)\n",
        "kan_rrmse,kan_ssim,kan_mse,kan_psnr,kan_decoded = gaim_metrics(test_loader, Autoencoder, kan, RNN_flag)\n",
        "test_ro_loader = torch.utils.data.DataLoader(test_dataset, batch_size=73, shuffle=False)\n",
        "kan_ro_rrmse,kan_ro_ssim,kan_ro_mse,kan_ro_psnr = rollout_metrics(test_ro_loader, Autoencoder, kan, RNN_flag)\n",
        "#mean metrics\n",
        "kan_avg_ssim = np.mean(kan_ssim)\n",
        "kan_avg_rrmse = np.mean(kan_rrmse)\n",
        "kan_avg_mse = np.mean(kan_mse)\n",
        "kan_avg_psnr = np.mean(kan_psnr)\n",
        "#mean rollout metircs\n",
        "kan_avg_ro_ssim = np.mean(kan_ro_ssim)\n",
        "kan_avg_ro_rrmse = np.mean(kan_ro_rrmse)\n",
        "kan_avg_ro_mse = np.mean(kan_ro_mse)\n",
        "kan_avg_ro_psnr = np.mean(kan_ro_psnr)\n",
        "\n",
        "\n",
        "print(f'Mean RRMSE: {kan_avg_rrmse:.6f}')\n",
        "print(f'Mean SSIM: {kan_avg_ssim:.6f}')\n",
        "print(f'Mean MSE: {kan_avg_mse:.6f}')\n",
        "print(f'Mean PSNR: {kan_avg_psnr:.6f}')\n",
        "\n",
        "print(f'Mean RRMSE rollout: {kan_avg_ro_rrmse:.6f}')\n",
        "print(f'Mean SSIM rollout: {kan_avg_ro_ssim:.6f}')\n",
        "print(f'Mean MSE rollout: {kan_avg_ro_mse:.6f}')\n",
        "print(f'Mean PSNR rollout: {kan_avg_ro_psnr:.6f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjUAn4wccAXn"
      },
      "source": [
        "MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UtuJ3tbFb_iy",
        "outputId": "81d93990-0de8-4828-e8cf-2c94883206b1"
      },
      "outputs": [],
      "source": [
        "input_dim = 5 * 16\n",
        "output_dim = 5 * 16\n",
        "input_dims = [input_dim,256,output_dim]  # Example hidden layers\n",
        "\n",
        "mlp = MLP(input_dims).to(device)\n",
        "RNN_flag = False\n",
        "print(mlp)\n",
        "mlp_save_path = content_path + '/mlp.pth'\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=1e-5, path=mlp_save_path)\n",
        "train(mlp,RNN_flag, train_loader,test_loader,early_stopping)\n",
        "mlp_rrmse,mlp_ssim,mlp_mse,mlp_psnr,mlp_decoded = gaim_metrics(test_loader, Autoencoder, mlp, RNN_flag)\n",
        "test_ro_loader = torch.utils.data.DataLoader(test_dataset, batch_size=73, shuffle=False)\n",
        "mlp_ro_rrmse,mlp_ro_ssim,mlp_ro_mse,mlp_ro_psnr = rollout_metrics(test_ro_loader, Autoencoder, mlp, RNN_flag)\n",
        "#mean metrics\n",
        "mlp_avg_ssim = np.mean(mlp_ssim)\n",
        "mlp_avg_rrmse = np.mean(mlp_rrmse)\n",
        "mlp_avg_mse = np.mean(mlp_mse)\n",
        "mlp_avg_psnr = np.mean(mlp_psnr)\n",
        "#mean rollout metircs\n",
        "mlp_avg_ro_ssim = np.mean(mlp_ro_ssim)\n",
        "mlp_avg_ro_rrmse = np.mean(mlp_ro_rrmse)\n",
        "mlp_avg_ro_mse = np.mean(mlp_ro_mse)\n",
        "mlp_avg_ro_psnr = np.mean(mlp_ro_psnr)\n",
        "\n",
        "\n",
        "print(f'Mean RRMSE: {mlp_avg_rrmse:.6f}')\n",
        "print(f'Mean SSIM: {mlp_avg_ssim:.6f}')\n",
        "print(f'Mean MSE: {mlp_avg_mse:.6f}')\n",
        "print(f'Mean PSNR: {kan_avg_psnr:.6f}')\n",
        "\n",
        "print(f'Mean RRMSE rollout: {mlp_avg_ro_rrmse:.6f}')\n",
        "print(f'Mean SSIM rollout: {mlp_avg_ro_ssim:.6f}')\n",
        "print(f'Mean MSE rollout: {mlp_avg_ro_mse:.6f}')\n",
        "print(f'Mean PSNR rollout: {mlp_avg_ro_psnr:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irRy5RSac4J0"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b-ai81bCc5hA",
        "outputId": "e111aa43-fd1a-459d-8d14-eadd408682d3"
      },
      "outputs": [],
      "source": [
        "lstm = LSTMModel(16,100,16,1,0.2).to(device)\n",
        "RNN_flag = True\n",
        "print(lstm)\n",
        "model_save_path = content_path + '/lstm.pth'\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=1e-5, path=model_save_path)\n",
        "train(lstm,RNN_flag, train_loader,test_loader,early_stopping)\n",
        "lstm_rrmse,lstm_ssim,lstm_mse,lstm_psnr,lstm_decoded = gaim_metrics(test_loader, Autoencoder, lstm, RNN_flag)\n",
        "test_ro_loader = torch.utils.data.DataLoader(test_dataset, batch_size=73, shuffle=False)\n",
        "lstm_ro_rrmse,lstm_ro_ssim,lstm_ro_mse,lstm_ro_psnr = rollout_metrics(test_ro_loader, Autoencoder, lstm, RNN_flag)\n",
        "#mean metrics\n",
        "lstm_avg_ssim = np.mean(lstm_ssim)\n",
        "lstm_avg_rrmse = np.mean(lstm_rrmse)\n",
        "lstm_avg_mse = np.mean(lstm_mse)\n",
        "lstm_avg_psnr = np.mean(lstm_psnr)\n",
        "#mean rollout metircs\n",
        "lstm_avg_ro_ssim = np.mean(lstm_ro_ssim)\n",
        "lstm_avg_ro_rrmse = np.mean(lstm_ro_rrmse)\n",
        "lstm_avg_ro_mse = np.mean(lstm_ro_mse)\n",
        "lstm_avg_ro_psnr = np.mean(lstm_ro_psnr)\n",
        "\n",
        "\n",
        "print(f'Mean RRMSE: {lstm_avg_rrmse:.6f}')\n",
        "print(f'Mean SSIM: {lstm_avg_ssim:.6f}')\n",
        "print(f'Mean MSE: {lstm_avg_mse:.6f}')\n",
        "print(f'Mean PSNR: {kan_avg_psnr:.6f}')\n",
        "\n",
        "print(f'Mean RRMSE rollout: {lstm_avg_ro_rrmse:.6f}')\n",
        "print(f'Mean SSIM rollout: {lstm_avg_ro_ssim:.6f}')\n",
        "print(f'Mean MSE rollout: {lstm_avg_ro_mse:.6f}')\n",
        "print(f'Mean PSNR rollout: {lstm_avg_ro_psnr:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsyAHzuzdfKa"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XsHuSOm8dgjG",
        "outputId": "ddc7f880-1bf3-420d-d3dc-bea036558435"
      },
      "outputs": [],
      "source": [
        "gru = GRUModel(16,100,16,1,0.2).to(device)\n",
        "RNN_flag = True\n",
        "print(gru)\n",
        "model_save_path = content_path + '/gru.pth'\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=1e-5, path=model_save_path)\n",
        "train(gru,RNN_flag, train_loader,test_loader,early_stopping)\n",
        "gru_rrmse,gru_ssim,gru_mse,gru_psnr,gru_decoded = gaim_metrics(test_loader, Autoencoder, gru, RNN_flag)\n",
        "test_ro_loader = torch.utils.data.DataLoader(test_dataset, batch_size=73, shuffle=False)\n",
        "gru_ro_rrmse,gru_ro_ssim,gru_ro_mse,gru_ro_psnr = rollout_metrics(test_ro_loader, Autoencoder, gru, RNN_flag)\n",
        "#mean metrics\n",
        "gru_avg_ssim = np.mean(gru_ssim)\n",
        "gru_avg_rrmse = np.mean(gru_rrmse)\n",
        "gru_avg_mse = np.mean(gru_mse)\n",
        "gru_avg_psnr = np.mean(gru_psnr)\n",
        "#mean rollout metircs\n",
        "gru_avg_ro_ssim = np.mean(gru_ro_ssim)\n",
        "gru_avg_ro_rrmse = np.mean(gru_ro_rrmse)\n",
        "gru_avg_ro_mse = np.mean(gru_ro_mse)\n",
        "gru_avg_ro_psnr = np.mean(gru_ro_psnr)\n",
        "\n",
        "\n",
        "print(f'Mean RRMSE: {gru_avg_rrmse:.6f}')\n",
        "print(f'Mean SSIM: {gru_avg_ssim:.6f}')\n",
        "print(f'Mean MSE: {gru_avg_mse:.6f}')\n",
        "print(f'Mean PSNR: {kan_avg_psnr:.6f}')\n",
        "\n",
        "print(f'Mean RRMSE rollout: {gru_avg_ro_rrmse:.6f}')\n",
        "print(f'Mean SSIM rollout: {gru_avg_ro_ssim:.6f}')\n",
        "print(f'Mean MSE rollout: {gru_avg_ro_mse:.6f}')\n",
        "print(f'Mean PSNR rollout: {gru_avg_ro_psnr:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "dDON6JmuRjG7",
        "outputId": "b5c40a54-d7b6-441d-cba4-50a5ed333ace"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "steps = range(len(kan_ro_ssim[:73]))  #  SSIM \n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(steps, kan_ro_ssim[:73], label='KAN', linewidth=1)\n",
        "plt.plot(steps, mlp_ro_ssim[:73], label='MLP', linewidth=0.9,linestyle = '--')\n",
        "plt.plot(steps, lstm_ro_ssim[:73], label='LSTM', linewidth=0.9,linestyle = '--')\n",
        "plt.plot(steps, gru_ro_ssim[:73], label='GRU', linewidth=0.9,linestyle = '--')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('SSIM')\n",
        "plt.title('SSIM over Steps')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(steps, kan_ro_rrmse[:73], label='KAN', linewidth=1)\n",
        "plt.plot(steps, mlp_ro_rrmse[:73], label='MLP', linewidth=0.9,linestyle = '--')\n",
        "plt.plot(steps, lstm_ro_rrmse[:73], label='LSTM', linewidth=0.9,linestyle = '--')\n",
        "plt.plot(steps, gru_ro_rrmse[:73], label='GRU', linewidth=0.9,linestyle = '--')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('RRMSE')\n",
        "\n",
        "plt.title('RRMSE over Steps')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "j7UTqrX-j70L",
        "outputId": "a07726a4-9835-4d39-de64-27162377e857"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "decoded_outputs = kan_decoded\n",
        "# Assuming your images are in the shape (128, 128, 3) and you want to visualize them\n",
        "steps = decoded_outputs.shape[1]  # Number of time steps\n",
        "channels = decoded_outputs.shape[2]  # Number of channels\n",
        "image_height, image_width = decoded_outputs.shape[3], decoded_outputs.shape[4]  # Image dimensions\n",
        "\n",
        "# Increase the figsize to make each subplot larger\n",
        "fig, axs = plt.subplots(channels * 2, steps, figsize=(20, 20))  # Adjust the figsize as needed\n",
        "\n",
        "for step in range(steps):\n",
        "    for channel in range(channels):\n",
        "        # Plot the original image (target)\n",
        "        axs[channel * 2, step].imshow(targets_test_oriimg[10, step, channel].cpu().numpy(), cmap='viridis')\n",
        "        axs[channel * 2, step].axis('off')\n",
        "        axs[channel * 2, step].set_title(f'Original - Step {step + 1}, Channel {channel + 1}')\n",
        "\n",
        "        # Plot the reconstructed image (output)\n",
        "        axs[channel * 2 + 1, step].imshow(decoded_outputs[10, step, channel].cpu().numpy(), cmap='viridis')\n",
        "        axs[channel * 2 + 1, step].axis('off')\n",
        "        axs[channel * 2 + 1, step].set_title(f'Reconstructed - Step {step + 1}, Channel {channel + 1}')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mk2k2PZGSOK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to normalize images\n",
        "def normalize_image(image):\n",
        "    min_val = np.min(image)\n",
        "    max_val = np.max(image)\n",
        "    normalized_image = (image - min_val) / (max_val - min_val)\n",
        "    return normalized_image\n",
        "\n",
        "# Function to combine images\n",
        "def combine_images(images, num_images, steps, channels):\n",
        "    combined_images = []\n",
        "    for i in range(num_images):\n",
        "        combined_set = []\n",
        "        for j in range(channels):\n",
        "            combined_input = np.concatenate([normalize_image(images[i, step, j]) for step in range(steps)], axis=1)\n",
        "            combined_set.append(combined_input)\n",
        "        combined_images.append(combined_set)\n",
        "    return combined_images\n",
        "\n",
        "# Function to visualize the original and decoded data\n",
        "def visualize_reconstruction(inputs, outputs, num_images=5):\n",
        "    inputs_combined = combine_images(inputs.cpu().numpy(), num_images, 5, 3)\n",
        "    outputs_combined = combine_images(outputs.cpu().numpy(), num_images, 5, 3)\n",
        "\n",
        "    fig, axes = plt.subplots(num_images, 2, figsize=(18, num_images * 6))\n",
        "\n",
        "    # Ensure axes is always a 2D array\n",
        "    if num_images == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        input_image = np.concatenate(inputs_combined[i], axis=0)\n",
        "        output_image = np.concatenate(outputs_combined[i], axis=0)\n",
        "        print(\"input_image.shape\", input_image.shape)\n",
        "        print(\"output_image.shape\", output_image.shape)\n",
        "\n",
        "        ax = axes[i, 0]\n",
        "        ax.imshow(input_image, cmap='viridis')\n",
        "        ax.set_title(f'Input 5 steps in u,v,h')\n",
        "        ax.axis('off')\n",
        "\n",
        "        ax = axes[i, 1]\n",
        "        ax.imshow(output_image, cmap='viridis')\n",
        "        ax.set_title(f'Output 5 steps in u,v,h')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Select a batch of test data for visualization\n",
        "count = 0\n",
        "for data in test_loader:\n",
        "    test_data = data\n",
        "    #show the later images\n",
        "    if count==20:\n",
        "      break\n",
        "    count+=1\n",
        "\n",
        "test_inputs, test_targets = test_data\n",
        "\n",
        "if RNN_flag == False:\n",
        "  flatten_inputs = test_inputs.reshape(test_inputs.shape[0],test_inputs.shape[1]*test_inputs.shape[2])\n",
        "  flatten_outputs = kan(flatten_inputs)\n",
        "  test_outputs = flatten_outputs.reshape(test_inputs.shape[0],test_inputs.shape[1],test_inputs.shape[2])\n",
        "elif RNN_flag == True:\n",
        "  test_outputs = kan(test_inputs)\n",
        "\n",
        "# Decode the latent vectors to original shape\n",
        "decoded_inputs = torch.zeros(test_inputs.size(0), test_inputs.size(1), 3, N, N).to(device)\n",
        "decoded_outputs = torch.zeros(test_outputs.size(0), test_outputs.size(1), 3, N, N).to(device)\n",
        "with torch.no_grad():\n",
        "    for i in range(test_inputs.size(0)):\n",
        "        for j in range(test_inputs.size(1)):\n",
        "            decoded_inputs[i, j] = Autoencoder.decoder(test_inputs[i, j].unsqueeze(0)).squeeze(0)\n",
        "            decoded_outputs[i, j] = Autoencoder.decoder(test_outputs[i, j].unsqueeze(0)).squeeze(0)\n",
        "\n",
        "# Visualize the first set of slices and their corresponding targets\n",
        "visualize_reconstruction(decoded_inputs, decoded_outputs, num_images=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
