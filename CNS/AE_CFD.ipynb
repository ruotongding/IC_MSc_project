{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "68SCx6Qh887N"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pylab import *\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5H-24xF9GP6",
        "outputId": "0a3edff8-c514-4262-d69d-f8a32d852d64"
      },
      "outputs": [],
      "source": [
        "content_path = 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bt9u4W_9k3r",
        "outputId": "6799b842-dbde-4bd8-fac2-37c4461f3913"
      },
      "outputs": [],
      "source": [
        "pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s4JStsB9lku",
        "outputId": "ea681ef1-49b6-47f4-bd40-c44746eda6d0"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "def inspect_hdf5_file(file_path):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        # Print all root level groups and datasets\n",
        "        def print_structure(name, obj):\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                print(\"1\")\n",
        "                print(f\"Dataset: {name}, Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
        "            elif isinstance(obj, h5py.Group):\n",
        "                print(\"2\")\n",
        "                print(f\"Group: {name}\")\n",
        "\n",
        "        f.visititems(print_structure)\n",
        "data_path = content_path+'/2D_CFD_Rand_M0.1_Eta0.01_Zeta0.01_periodic_128_Train.hdf5'\n",
        "# Example usage\n",
        "inspect_hdf5_file(data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwP58LTU-bbO",
        "outputId": "96cefab0-038f-42fd-ab30-bd54214d2aa2"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def get_norm_val(file_path, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        # Load the datasets\n",
        "        Vx = f['Vx']\n",
        "        Vy = f['Vy']\n",
        "        Vx_max = -999\n",
        "        Vx_min = 999\n",
        "        Vy_max = -999\n",
        "        Vy_min = 999\n",
        "\n",
        "\n",
        "\n",
        "        num_sequences = Vx.shape[0]\n",
        "        for i in range(num_sequences):\n",
        "            Vx_seq = Vx[i].reshape(21, 1, 128, 128)\n",
        "\n",
        "            Vy_seq = Vy[i].reshape(21, 1, 128, 128)\n",
        "\n",
        "            Vx_max = max(Vx_max, np.max(Vx_seq))\n",
        "            Vx_min = min(Vx_min, np.min(Vx_seq))\n",
        "            Vy_max = max(Vy_max, np.max(Vy_seq))\n",
        "            Vy_min = min(Vy_min, np.min(Vy_seq))\n",
        "\n",
        "            # Save the combined sequence as a .npy file\n",
        "            #np.save(os.path.join(output_dir, f'sequence_{i}.npy'), combined_seq)\n",
        "        return Vx_max, Vx_min, Vy_max, Vy_min\n",
        "\n",
        "# Example usage\n",
        "\n",
        "output_dir = content_path+'/CFD'\n",
        "Vx_max, Vx_min, Vy_max, Vy_min = get_norm_val(data_path, output_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvLUdswO12Ga",
        "outputId": "a6d3c5d3-faf8-4bb6-f4d4-100cc77d347e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def save_in_npy(file_path, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        # Load the datasets\n",
        "        Vx = f['Vx']\n",
        "        Vy = f['Vy']\n",
        "        num_sequences = Vx.shape[0]\n",
        "        for i in range(num_sequences):\n",
        "            Vx_seq = Vx[i].reshape(21, 1, 128, 128)\n",
        "            Vy_seq = Vy[i].reshape(21, 1, 128, 128)\n",
        "            Vx_seq = (Vx_seq - np.min(Vx_seq)) / (np.max(Vx_seq) - np.min(Vx_seq))\n",
        "            Vy_seq = (Vy_seq - np.min(Vy_seq)) / (np.max(Vy_seq) - np.min(Vy_seq))\n",
        "            combined_seq = np.concatenate([Vx_seq, Vy_seq], axis=1)\n",
        "            np.save(os.path.join(output_dir, f'sequence_{i}.npy'), combined_seq)\n",
        "        print(\"save done\")\n",
        "\n",
        "# Example usage\n",
        "\n",
        "output_dir = content_path+'/CFD_2_channels'\n",
        "save_in_npy(data_path, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Bt-ih9--V3",
        "outputId": "4f5aab79-103b-4b85-858b-21ca7b61ea6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import re\n",
        "# import time\n",
        "\n",
        "class NPYDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (string): Directory with all the .npy files.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.file_names = sorted([f for f in os.listdir(data_dir) if f.endswith('.npy')],\n",
        "                                 key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
        "\n",
        "        print(\"len file_names\",self.file_names)\n",
        "        self.num_frames_per_file = 21  # Number of frames per sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total number of samples (frames) across all files\n",
        "        return len(self.file_names) * self.num_frames_per_file\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"asdsa\")\n",
        "        # t1 = time.time()\n",
        "        # Calculate the corresponding file index and frame index\n",
        "        # Find the corresponding file and frame index within that file\n",
        "        file_idx = idx // self.num_frames_per_file\n",
        "        frame_idx = idx % self.num_frames_per_file\n",
        "        \n",
        "        \n",
        "\n",
        "        # Load the corresponding .npy file\n",
        "        npy_file = os.path.join(self.data_dir, self.file_names[file_idx])\n",
        "        sequence = np.load(npy_file, mmap_mode='r') # Load the entire sequence from the .npy file\n",
        "       \n",
        "        # Extract the specific frame from the sequence\n",
        "        frame = sequence[frame_idx]  # Shape: (4, 128, 128)\n",
        "\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "        # t4 = time.time()\n",
        "        \n",
        "\n",
        "\n",
        "        return torch.tensor(frame, dtype=torch.float32)\n",
        "\n",
        "                 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "output_dir = content_path+'/CFD_2_channels'\n",
        "print(output_dir)\n",
        "dataset = NPYDataset(data_dir=output_dir)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "print(\"len dataloader\",len(dataloader))\n",
        "print(\"len dataset\",len(dataset))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PpiFoBzDVpgn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class KANLinear(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        enable_standalone_scale_spline=True,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = (\n",
        "            (\n",
        "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                + grid_range[0]\n",
        "            )\n",
        "            .expand(in_features, -1)\n",
        "            .contiguous()\n",
        "        )\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = torch.nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = torch.nn.Parameter(\n",
        "                torch.Tensor(out_features, in_features)\n",
        "            )\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = (\n",
        "                (\n",
        "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
        "                    - 1 / 2\n",
        "                )\n",
        "                * self.scale_noise\n",
        "                / self.grid_size\n",
        "            )\n",
        "            self.spline_weight.data.copy_(\n",
        "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
        "                * self.curve2coeff(\n",
        "                    self.grid.T[self.spline_order : -self.spline_order],\n",
        "                    noise,\n",
        "                )\n",
        "            )\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
        "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the B-spline bases for the given input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "\n",
        "        grid: torch.Tensor = (\n",
        "            self.grid\n",
        "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the coefficients of the curve that interpolates the given points.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "\n",
        "        A = self.b_splines(x).transpose(\n",
        "            0, 1\n",
        "        )  # (in_features, batch_size, grid_size + spline_order)\n",
        "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
        "        solution = torch.linalg.lstsq(\n",
        "            A, B\n",
        "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
        "        result = solution.permute(\n",
        "            2, 0, 1\n",
        "        )  # (out_features, in_features, grid_size + spline_order)\n",
        "\n",
        "        assert result.size() == (\n",
        "            self.out_features,\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        return self.spline_weight * (\n",
        "            self.spline_scaler.unsqueeze(-1)\n",
        "            if self.enable_standalone_scale_spline\n",
        "            else 1.0\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.size(-1) == self.in_features\n",
        "        original_shape = x.shape\n",
        "        x = x.reshape(-1, self.in_features)\n",
        "\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.scaled_spline_weight.view(self.out_features, -1),\n",
        "        )\n",
        "        output = base_output + spline_output\n",
        "\n",
        "        output = output.reshape(*original_shape[:-1], self.out_features)\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "\n",
        "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
        "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
        "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
        "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
        "        unreduced_spline_output = unreduced_spline_output.permute(\n",
        "            1, 0, 2\n",
        "        )  # (batch, in, out)\n",
        "\n",
        "        # sort each channel individually to collect data distribution\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[\n",
        "            torch.linspace(\n",
        "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (\n",
        "            torch.arange(\n",
        "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
        "            ).unsqueeze(1)\n",
        "            * uniform_step\n",
        "            + x_sorted[0]\n",
        "            - margin\n",
        "        )\n",
        "\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.concatenate(\n",
        "            [\n",
        "                grid[:1]\n",
        "                - uniform_step\n",
        "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
        "                grid,\n",
        "                grid[-1:]\n",
        "                + uniform_step\n",
        "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        \"\"\"\n",
        "        Compute the regularization loss.\n",
        "\n",
        "        This is a dumb simulation of the original L1 regularization as stated in the\n",
        "        paper, since the original one requires computing absolutes and entropy from the\n",
        "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
        "        behind the F.linear function if we want an memory efficient implementation.\n",
        "\n",
        "        The L1 regularization is now computed as mean absolute value of the spline\n",
        "        weights. The authors implementation also includes this term in addition to the\n",
        "        sample-based regularization.\n",
        "        \"\"\"\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return (\n",
        "            regularize_activation * regularization_loss_activation\n",
        "            + regularize_entropy * regularization_loss_entropy\n",
        "        )\n",
        "\n",
        "\n",
        "class KAN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers_hidden,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KAN, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
        "            self.layers.append(\n",
        "                KANLinear(\n",
        "                    in_features,\n",
        "                    out_features,\n",
        "                    grid_size=grid_size,\n",
        "                    spline_order=spline_order,\n",
        "                    scale_noise=scale_noise,\n",
        "                    scale_base=scale_base,\n",
        "                    scale_spline=scale_spline,\n",
        "                    base_activation=base_activation,\n",
        "                    grid_eps=grid_eps,\n",
        "                    grid_range=grid_range,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, update_grid=False):\n",
        "        for layer in self.layers:\n",
        "            if update_grid:\n",
        "                layer.update_grid(x)\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        return sum(\n",
        "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
        "            for layer in self.layers\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TPZ2uH1TaS2_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PowerfulAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(PowerfulAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1),  # (128, 128, 4) -> (64, 64, 32)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # (64, 64, 32) -> (32, 32, 64)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # (32, 32, 64) -> (16, 16, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # (16, 16, 128) -> (8, 8, 256)\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            #nn.Linear(8*8*256, latent_dim)  # (8*8*256) -> (latent_dim)\n",
        "            KANLinear(8*8*256, latent_dim)\n",
        "\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8*8*256),  # (latent_dim) -> (8*8*256)\n",
        "            nn.Unflatten(1, (256, 8, 8)),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (8, 8, 256) -> (16, 16, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (16, 16, 128) -> (32, 32, 64)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # (32, 32, 64) -> (64, 64, 32)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 2, kernel_size=4, stride=2, padding=1), # (64, 64, 32) -> (128, 128, 4)\n",
        "            nn.Sigmoid()  # Use sigmoid if your image pixels are in the range [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        return x_reconstructed,z\n",
        "\n",
        "def loss_function(reconstructed, original, latent, lambda_reg=0.1):\n",
        "    reconstruction_loss = nn.MSELoss()(reconstructed, original)\n",
        "    latent_reg_loss = torch.mean(torch.norm(latent, p=2, dim=1))\n",
        "    total_loss = reconstruction_loss + lambda_reg * latent_reg_loss\n",
        "    return total_loss, reconstruction_loss, latent_reg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_-ubk3iyRLs",
        "outputId": "87e84fd4-3a4a-4221-dc42-7050f99861d7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "model_save_path = content_path + '/model/AE_CFD_KAN_2_channels.pth'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# dataset = torch.tensor(dataset, dtype=torch.float32)\n",
        "print(\"dataset\",dataset)\n",
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers =4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers = 4)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "model = PowerfulAutoencoder(latent_dim=256).to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "lambda_reg = 1e-6\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_latent_loss = 0\n",
        "    step = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)  # Move data to GPU\n",
        "      \n",
        "        # Forward pass\n",
        "        output,latent = model(data)\n",
        "        # loss = criterion(output, data)\n",
        "        loss, reconstruction_loss, latent_reg_loss = loss_function(output,data,latent,lambda_reg=lambda_reg)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_latent_loss += latent_reg_loss.item()\n",
        "        step+=1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_latent_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_latent_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)  # Move data to GPU\n",
        "            #print(\"input_data.shape\",data.shape)\n",
        "\n",
        "            # Forward pass\n",
        "            output,latent = model(data)\n",
        "            # loss = criterion(output, data)\n",
        "            loss, reconstruction_loss, latent_reg_loss = loss_function(output,data,latent,lambda_reg=lambda_reg)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            test_latent_loss += latent_reg_loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_latent_loss /= len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f},Train Latent loss: {train_latent_loss:.6f}, Test Latent Loss: {test_latent_loss:.6f}')\n",
        "\n",
        "\n",
        "print('Training complete')\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# Plot the training and testing losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, len(train_losses)+1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Testing Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mpmOoGj4E6Oy",
        "outputId": "bf3aa372-c6c0-46ef-9cc0-8d0eab7544ca"
      },
      "outputs": [],
      "source": [
        "from matplotlib import colors\n",
        "def visualize_reconstruction(model, data_loader, num_images=5):\n",
        "    model.eval()\n",
        "    # data_iter = iter(data_loader)\n",
        "    count = 0\n",
        "    for data in data_loader:\n",
        "        original_data = data.to(device)\n",
        "        #see results of a batch\n",
        "        if count==3:\n",
        "            break\n",
        "        count+=1\n",
        "    with torch.no_grad():\n",
        "        reconstructed_data,_ = model(original_data)\n",
        "  \n",
        "\n",
        "    original_data = original_data.cpu().numpy()\n",
        "    reconstructed_data = reconstructed_data.cpu().numpy()\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(num_images, 4, figsize=(18, num_images * 2))\n",
        "    for i in range(num_images):\n",
        "        for j in range(2):\n",
        "            ax = axes[i, j*2]\n",
        "\n",
        "            ax.imshow(original_data[i, j], cmap='viridis')\n",
        "\n",
        "            ax.set_title(f'Original - Channel {j+1}')\n",
        "            ax.axis('off')\n",
        "\n",
        "            ax = axes[i, j*2 + 1]\n",
        "            ax.imshow(reconstructed_data[i, j], cmap='viridis')\n",
        "            ax.set_title(f'Reconstructed - Channel {j+1}')\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "   \n",
        "\n",
        "# Visualize some reconstructions\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "visualize_reconstruction(model, test_loader, num_images=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHyd_hkMbXMr"
      },
      "outputs": [],
      "source": [
        "pip install scikit-image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFl-e94aafCE",
        "outputId": "456a9560-18e4-40f3-c717-e742cac8a49a"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "# Define RRMSE and SSIM functions\n",
        "def rrmse(img1, img2):\n",
        "    return np.sqrt(np.mean((img1 - img2) ** 2)) / np.sqrt(np.mean(img1 ** 2))\n",
        "\n",
        "def compute_ssim(img1, img2):\n",
        "    return ssim(img1, img2, data_range=img2.max() - img2.min(), multichannel=True,win_size=5, channel_axis=-1)\n",
        "\n",
        "# Compute RRMSE and SSIM for the entire test dataset\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    rrmse_values = []\n",
        "    ssim_values = []\n",
        "    mse_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            reconstructed_data,_ = model(data)\n",
        "            original_data = data.cpu().numpy()\n",
        "            reconstructed_data = reconstructed_data.cpu().numpy()\n",
        "\n",
        "            for i in range(original_data.shape[0]):\n",
        "\n",
        "                original_img = original_data[i].transpose(1, 2, 0)\n",
        "\n",
        "                reconstructed_img = reconstructed_data[i].transpose(1, 2, 0)\n",
        "\n",
        "                rrmse_value = rrmse(original_img, reconstructed_img)\n",
        "\n",
        "                ssim_value = compute_ssim(original_img, reconstructed_img)\n",
        "\n",
        "                mse = np.mean((original_img - reconstructed_img) ** 2)\n",
        "               \n",
        "\n",
        "                rrmse_values.append(rrmse_value)\n",
        "                ssim_values.append(ssim_value)\n",
        "                mse_values.append(mse)\n",
        "\n",
        "    mean_rrmse = np.mean(rrmse_values)\n",
        "    mean_ssim = np.mean(ssim_values)\n",
        "    mean_mse = np.mean(mse_values)\n",
        "    print(\"metric for AE with KAN\")\n",
        "    #print(\"metric for AE\")\n",
        "    print(f'Mean RRMSE: {mean_rrmse:.6f}')\n",
        "    print(f'Mean SSIM: {mean_ssim:.6f}')\n",
        "    print(f'Mean MSE: {mean_mse:.6f}')\n",
        "\n",
        "    return mean_rrmse, mean_ssim,mean_mse\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "\n",
        "mean_rrmse, mean_ssim,mean_mse = evaluate_model(model, test_loader)\n",
        "print(f'Mean RRMSE: {mean_rrmse:.6f}')\n",
        "print(f'Mean SSIM: {mean_ssim:.6f}')\n",
        "print(f'Mean MSE: {mean_mse:.6f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
